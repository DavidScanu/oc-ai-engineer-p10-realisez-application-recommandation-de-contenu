# backend/scripts/analyze_articles_metadata.py
import pandas as pd
from pathlib import Path

def analyze_articles_metadata():
    """Analyse complÃ¨te du fichier articles_metadata.csv"""
    
    data_path = Path("backend/data/articles_metadata.csv")
    
    if not data_path.exists():
        print("âŒ Fichier articles_metadata.csv non trouvÃ©")
        return
    
    print("ğŸ“Š ANALYSE DU FICHIER ARTICLES_METADATA.CSV")
    print("=" * 60)
    
    # Charger les donnÃ©es
    df = pd.read_csv(data_path)
    print(f"ğŸ” Dataset chargÃ©: {len(df):,} articles")
    
    # ========== INFORMATIONS GÃ‰NÃ‰RALES ==========
    print("\n" + "=" * 40)
    print("ğŸ“‹ INFORMATIONS GÃ‰NÃ‰RALES")
    print("=" * 40)
    
    print(f"ğŸ“„ Nombre total d'articles: {len(df):,}")
    print(f"ğŸ“Š Nombre de colonnes: {len(df.columns)}")
    print(f"ğŸ“ Colonnes: {list(df.columns)}")
    print(f"ğŸ’¾ Taille en mÃ©moire: {df.memory_usage(deep=True).sum() / 1024 / 1024:.1f} MB")
    
    # Valeurs manquantes
    missing = df.isnull().sum()
    if missing.sum() > 0:
        print(f"\nâš ï¸  Valeurs manquantes:")
        for col, count in missing.items():
            if count > 0:
                print(f"   - {col}: {count:,} ({count/len(df)*100:.1f}%)")
    else:
        print("\nâœ… Aucune valeur manquante")
    
    # ========== ANALYSE DES ARTICLES ==========
    print("\n" + "=" * 40)
    print("ğŸ“„ ANALYSE DES ARTICLES")
    print("=" * 40)
    
    print(f"ğŸ†” Article ID min: {df['article_id'].min():,}")
    print(f"ğŸ†” Article ID max: {df['article_id'].max():,}")
    print(f"ğŸ”¢ Nombre d'IDs uniques: {df['article_id'].nunique():,}")
    
    if df['article_id'].nunique() != len(df):
        duplicates = len(df) - df['article_id'].nunique()
        print(f"âš ï¸  Doublons dÃ©tectÃ©s: {duplicates:,}")
    else:
        print("âœ… Tous les article_id sont uniques")
    
    # ========== ANALYSE DES CATÃ‰GORIES ==========
    print("\n" + "=" * 40)
    print("ğŸ·ï¸  ANALYSE DES CATÃ‰GORIES")
    print("=" * 40)
    
    category_counts = df['category_id'].value_counts().head(10)
    print(f"ğŸ“Š Nombre de catÃ©gories uniques: {df['category_id'].nunique():,}")
    print(f"ğŸ“ˆ Distribution des top 10 catÃ©gories:")
    for cat_id, count in category_counts.items():
        percentage = count / len(df) * 100
        print(f"   - CatÃ©gorie {cat_id}: {count:,} articles ({percentage:.1f}%)")
    
    # Statistiques des catÃ©gories
    print(f"\nğŸ“Š Statistiques des catÃ©gories:")
    print(f"   - Moyenne d'articles par catÃ©gorie: {len(df) / df['category_id'].nunique():.1f}")
    print(f"   - MÃ©diane d'articles par catÃ©gorie: {df['category_id'].value_counts().median():.1f}")
    print(f"   - CatÃ©gorie la plus reprÃ©sentÃ©e: {df['category_id'].mode().iloc[0]} ({category_counts.iloc[0]:,} articles)")
    
    # ========== ANALYSE DES Ã‰DITEURS ==========
    print("\n" + "=" * 40)
    print("ğŸ“° ANALYSE DES Ã‰DITEURS")
    print("=" * 40)
    
    publisher_counts = df['publisher_id'].value_counts().head(10)
    print(f"ğŸ¢ Nombre d'Ã©diteurs uniques: {df['publisher_id'].nunique():,}")
    print(f"ğŸ“ˆ Distribution des top 10 Ã©diteurs:")
    for pub_id, count in publisher_counts.items():
        percentage = count / len(df) * 100
        print(f"   - Ã‰diteur {pub_id}: {count:,} articles ({percentage:.1f}%)")
    
    # ========== ANALYSE DES DATES ==========
    print("\n" + "=" * 40)
    print("ğŸ“… ANALYSE DES DATES DE CRÃ‰ATION")
    print("=" * 40)
    
    # Convertir les timestamps en dates
    df['created_date'] = pd.to_datetime(df['created_at_ts'], unit='ms')
    
    print(f"ğŸ“… Date la plus ancienne: {df['created_date'].min().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“… Date la plus rÃ©cente: {df['created_date'].max().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“Š PÃ©riode couverte: {(df['created_date'].max() - df['created_date'].min()).days:,} jours")
    
    # Articles par annÃ©e
    df['year'] = df['created_date'].dt.year
    yearly_counts = df['year'].value_counts().sort_index()
    print(f"\nğŸ“ˆ Articles par annÃ©e:")
    for year, count in yearly_counts.items():
        print(f"   - {year}: {count:,} articles")
    
    # Articles par mois (derniÃ¨re annÃ©e)
    latest_year = df['year'].max()
    latest_year_data = df[df['year'] == latest_year]
    if len(latest_year_data) > 0:
        monthly_counts = latest_year_data['created_date'].dt.month.value_counts().sort_index()
        print(f"\nğŸ“ˆ Articles par mois en {latest_year}:")
        months = ['Jan', 'FÃ©v', 'Mar', 'Avr', 'Mai', 'Juin',
                  'Juil', 'AoÃ»t', 'Sep', 'Oct', 'Nov', 'DÃ©c']
        for month, count in monthly_counts.items():
            print(f"   - {months[month-1]}: {count:,} articles")
    
    # ========== ANALYSE DU NOMBRE DE MOTS ==========
    print("\n" + "=" * 40)
    print("ğŸ“ ANALYSE DU NOMBRE DE MOTS")
    print("=" * 40)
    
    words_stats = df['words_count'].describe()
    print(f"ğŸ“Š Statistiques du nombre de mots:")
    print(f"   - Minimum: {words_stats['min']:.0f} mots")
    print(f"   - Maximum: {words_stats['max']:.0f} mots")
    print(f"   - Moyenne: {words_stats['mean']:.0f} mots")
    print(f"   - MÃ©diane: {words_stats['50%']:.0f} mots")
    print(f"   - Ã‰cart-type: {words_stats['std']:.0f} mots")
    
    # Distribution par tranches
    print(f"\nğŸ“ˆ Distribution par tranches de mots:")
    bins = [0, 100, 200, 300, 500, 1000, float('inf')]
    labels = ['0-100', '101-200', '201-300', '301-500', '501-1000', '1000+']
    df['word_range'] = pd.cut(df['words_count'], bins=bins, labels=labels, right=True)
    word_distribution = df['word_range'].value_counts().sort_index()
    
    for range_label, count in word_distribution.items():
        percentage = count / len(df) * 100
        print(f"   - {range_label} mots: {count:,} articles ({percentage:.1f}%)")
    
    # ========== CORRÃ‰LATIONS ET INSIGHTS ==========
    print("\n" + "=" * 40)
    print("ğŸ” INSIGHTS ET CORRÃ‰LATIONS")
    print("=" * 40)
    
    # Nombre moyen de mots par catÃ©gorie
    avg_words_by_category = df.groupby('category_id')['words_count'].mean().sort_values(ascending=False).head(5)
    print(f"ğŸ“Š Top 5 des catÃ©gories avec le plus de mots en moyenne:")
    for cat_id, avg_words in avg_words_by_category.items():
        print(f"   - CatÃ©gorie {cat_id}: {avg_words:.0f} mots en moyenne")
    
    # Nombre moyen de mots par Ã©diteur
    avg_words_by_publisher = df.groupby('publisher_id')['words_count'].mean().sort_values(ascending=False).head(5)
    print(f"\nğŸ“Š Top 5 des Ã©diteurs avec le plus de mots en moyenne:")
    for pub_id, avg_words in avg_words_by_publisher.items():
        print(f"   - Ã‰diteur {pub_id}: {avg_words:.0f} mots en moyenne")
    
    # Ã‰volution du nombre de mots dans le temps
    monthly_avg_words = df.groupby(df['created_date'].dt.to_period('M'))['words_count'].mean()
    if len(monthly_avg_words) > 1:
        trend = "croissant" if monthly_avg_words.iloc[-1] > monthly_avg_words.iloc[0] else "dÃ©croissant"
        print(f"\nğŸ“ˆ Tendance du nombre de mots: {trend}")
        print(f"   - Premier mois: {monthly_avg_words.iloc[0]:.0f} mots en moyenne")
        print(f"   - Dernier mois: {monthly_avg_words.iloc[-1]:.0f} mots en moyenne")
    
    # ========== RÃ‰SUMÃ‰ ==========
    print("\n" + "=" * 60)
    print("ğŸ“‹ RÃ‰SUMÃ‰ EXÃ‰CUTIF")
    print("=" * 60)
    
    print(f"âœ… Dataset de {len(df):,} articles sur {(df['created_date'].max() - df['created_date'].min()).days:,} jours")
    print(f"âœ… {df['category_id'].nunique():,} catÃ©gories diffÃ©rentes")
    print(f"âœ… {df['publisher_id'].nunique():,} Ã©diteurs diffÃ©rents")
    print(f"âœ… Articles de {words_stats['min']:.0f} Ã  {words_stats['max']:.0f} mots ({words_stats['mean']:.0f} en moyenne)")
    print(f"âœ… PÃ©riode: {df['created_date'].min().strftime('%Y-%m-%d')} Ã  {df['created_date'].max().strftime('%Y-%m-%d')}")
    
    # DÃ©tection d'anomalies potentielles
    print(f"\nğŸ” POINTS D'ATTENTION:")
    
    # Articles trÃ¨s courts ou trÃ¨s longs
    very_short = len(df[df['words_count'] < 50])
    very_long = len(df[df['words_count'] > 2000])
    
    if very_short > 0:
        print(f"âš ï¸  {very_short:,} articles trÃ¨s courts (< 50 mots) - {very_short/len(df)*100:.1f}%")
    
    if very_long > 0:
        print(f"âš ï¸  {very_long:,} articles trÃ¨s longs (> 2000 mots) - {very_long/len(df)*100:.1f}%")
    
    # Concentration des catÃ©gories
    top_category_pct = category_counts.iloc[0] / len(df) * 100
    if top_category_pct > 30:
        print(f"âš ï¸  La catÃ©gorie dominante reprÃ©sente {top_category_pct:.1f}% des articles")
    
    # Concentration des Ã©diteurs
    top_publisher_pct = publisher_counts.iloc[0] / len(df) * 100
    if top_publisher_pct > 50:
        print(f"âš ï¸  L'Ã©diteur dominant reprÃ©sente {top_publisher_pct:.1f}% des articles")
    
    return {
        'total_articles': len(df),
        'categories': df['category_id'].nunique(),
        'publishers': df['publisher_id'].nunique(),
        'date_range': (df['created_date'].min(), df['created_date'].max()),
        'words_stats': words_stats.to_dict(),
        'top_categories': category_counts.head(5).to_dict(),
        'top_publishers': publisher_counts.head(5).to_dict()
    }

if __name__ == "__main__":
    stats = analyze_articles_metadata()